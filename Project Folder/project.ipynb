{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fca11ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from pyspark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from requests) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement gzip (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for gzip\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement io (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for io\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (2.3.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from matplotlib) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "%pip install requests\n",
    "%pip install gzip\n",
    "%pip install io\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9d61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    \"https://datasets.imdbws.com/name.basics.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.akas.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.basics.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.crew.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.episode.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.principals.tsv.gz\",\n",
    "    \"https://datasets.imdbws.com/title.ratings.tsv.gz\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78cdfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/19 16:47:18 WARN Utils: Your hostname, MacBook-Pro-de-Ethan.local, resolves to a loopback address: 127.0.0.1; using 10.15.29.15 instead (on interface en0)\n",
      "25/11/19 16:47:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/19 16:47:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading name_basics from https://datasets.imdbws.com/name.basics.tsv.gz...\n",
      "14882055\n",
      "Loading title_akas from https://datasets.imdbws.com/title.akas.tsv.gz...\n",
      "53989582\n",
      "Loading title_basics from https://datasets.imdbws.com/title.basics.tsv.gz...\n",
      "12068991\n",
      "Loading title_crew from https://datasets.imdbws.com/title.crew.tsv.gz...\n",
      "12071057\n",
      "Loading title_episode from https://datasets.imdbws.com/title.episode.tsv.gz...\n",
      "9301107\n",
      "Loading title_principals from https://datasets.imdbws.com/title.principals.tsv.gz...\n",
      "95997473\n",
      "Loading title_ratings from https://datasets.imdbws.com/title.ratings.tsv.gz...\n",
      "1625494\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import gzip\n",
    "import io\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IMDB Data Loading\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load each dataset from sources\n",
    "dataframes = {}\n",
    "dataset_names = [\n",
    "    \"name_basics\",\n",
    "    \"title_akas\",\n",
    "    \"title_basics\",\n",
    "    \"title_crew\",\n",
    "    \"title_episode\",\n",
    "    \"title_principals\",\n",
    "    \"title_ratings\"\n",
    "]\n",
    "\n",
    "for url, name in zip(sources, dataset_names):\n",
    "    print(f\"Loading {name} from {url}...\")\n",
    "    \n",
    "    # Download and decompress the data\n",
    "    response = requests.get(url)\n",
    "    decompressed_data = gzip.decompress(response.content).decode('utf-8')\n",
    "    \n",
    "    # Convert to pandas DataFrame first \n",
    "    lines = decompressed_data.strip().split('\\n')\n",
    "    print(len(lines))\n",
    "    dataframes[name] = lines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afaff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 16:51:04 ERROR Inbox: An error happened while processing message in the inbox for LocalSchedulerBackendEndpoint\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3537)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1886)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1795)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1192)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:122)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:543)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:507)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager$$Lambda$2838/0x000000a001e7d770.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:242)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:483)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:413)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:408)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2829/0x000000a001e753f0.apply(Unknown Source)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:408)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2827/0x000000a001e75000.apply$mcVI$sp(Unknown Source)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:398)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$19(TaskSchedulerImpl.scala:579)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$19$adapted(TaskSchedulerImpl.scala:574)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2826/0x000000a001e778c0.apply(Unknown Source)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:574)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:551)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$805/0x000000a001526000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "Exception in thread \"dispatcher-event-loop-4\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3537)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:130)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1886)\n",
      "\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1795)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1192)\n",
      "\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:122)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:543)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:507)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager$$Lambda$2838/0x000000a001e7d770.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:242)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:483)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:413)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:408)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2829/0x000000a001e753f0.apply(Unknown Source)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:408)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2827/0x000000a001e75000.apply$mcVI$sp(Unknown Source)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:192)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:398)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$19(TaskSchedulerImpl.scala:579)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$19$adapted(TaskSchedulerImpl.scala:574)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$2826/0x000000a001e778c0.apply(Unknown Source)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:574)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:551)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$805/0x000000a001526000.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 0) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/ethansmadja/.pyenv/versions/3.13.3/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function JavaObject.__init__.<locals>.<lambda> at 0x4dfb46840>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages/py4j/java_gateway.py\", line 1399, in <lambda>\n",
      "    lambda wr, cc=self._gateway_client, id=self._target_id:\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "spark_dataframes = {}\n",
    "\n",
    "for name, lines in dataframes.items():\n",
    "    # Create RDD from the text data\n",
    "    rdd = spark.sparkContext.parallelize(lines, numSlices=10)\n",
    "    \n",
    "    # Parse header and data\n",
    "    header = lines[0].split('\\t')\n",
    "    data_lines = lines[1:]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \"\\t\") \\\n",
    "        .option(\"nullValue\", \"\\\\N\") \\\n",
    "        .csv(rdd)\n",
    "    \n",
    "    spark_dataframes[name] = df\n",
    "    print(f\"Loaded {name}: {df.count()} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# Access individual dataframes like: spark_dataframes['name_basics'], spark_dataframes['title_ratings'], etc.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
