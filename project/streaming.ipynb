{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Event Stream Processing\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a real-time stream processing job that tracks Wikipedia edit events for five entities from the IMDB dataset. We use the Wikimedia EventStreams API, which provides a continuous stream of events about changes happening across all Wikimedia projects (including Wikipedia).\n",
    "\n",
    "### Entities Tracked\n",
    "\n",
    "We track the following five IMDB-related entities:\n",
    "\n",
    "1. **Christopher Nolan** - Famous director (Inception, The Dark Knight, Interstellar)\n",
    "2. **The Godfather** - Classic movie, frequently referenced and edited\n",
    "3. **Quentin Tarantino** - Influential director/writer\n",
    "4. **Science Fiction** - Genre page with frequent activity\n",
    "5. **Academy Awards** - Major film awards, high edit activity\n",
    "\n",
    "### Metrics Collected\n",
    "\n",
    "For each tracked entity, we collect:\n",
    "- Total number of edits\n",
    "- Edit timestamps\n",
    "- User who made the edit\n",
    "- Whether the edit was made by a bot\n",
    "- Size change of the edit (bytes added/removed)\n",
    "\n",
    "### Alert System\n",
    "\n",
    "An alert is triggered when:\n",
    "- **Large edits** occur (> 500 bytes changed) - these could indicate vandalism or major content changes\n",
    "- **Bot edits** are detected - automated changes that might need review\n",
    "- **Rapid successive edits** - multiple edits within a short timeframe (potential edit war)\n",
    "\n",
    "### Output Structure\n",
    "\n",
    "The system outputs to two JSON files:\n",
    "1. `wiki_events.json` - All tracked events with metrics\n",
    "2. `wiki_alerts.json` - Alert events requiring attention\n",
    "\n",
    "Each event record contains:\n",
    "```json\n",
    "{\n",
    "    \"entity\": \"string - the tracked entity name\",\n",
    "    \"timestamp\": \"ISO 8601 timestamp\",\n",
    "    \"user\": \"string - username who made the edit\",\n",
    "    \"is_bot\": \"boolean - whether the user is a bot\",\n",
    "    \"title\": \"string - Wikipedia page title\",\n",
    "    \"comment\": \"string - edit comment/summary\",\n",
    "    \"size_change\": \"integer - bytes added/removed\",\n",
    "    \"wiki\": \"string - which wiki (e.g., enwiki)\",\n",
    "    \"revision_id\": \"integer - revision ID\"\n",
    "}\n",
    "```\n",
    "\n",
    "Alert records additionally contain:\n",
    "```json\n",
    "{\n",
    "    \"alert_type\": \"string - LARGE_EDIT, BOT_EDIT, or RAPID_EDIT\",\n",
    "    \"alert_reason\": \"string - detailed explanation\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, we install and import the necessary libraries for stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sseclient-py\n",
      "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: requests in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethansmadja/github/Big-Data-Processing-Labs-1/.venv/lib/python3.13/site-packages (from requests) (2025.11.12)\n",
      "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: sseclient-py\n",
      "Successfully installed sseclient-py-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install sseclient-py requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from sseclient import SSEClient\n",
    "import threading\n",
    "import time\n",
    "\n",
    "print(\"Dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Tracked Entities\n",
    "\n",
    "We define five entities from the IMDB dataset that have corresponding Wikipedia pages. These entities were chosen because:\n",
    "- They have active Wikipedia pages with regular edits\n",
    "- They represent different categories (directors, movies, genres, awards)\n",
    "- They are popular enough to generate sufficient streaming events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking 5 entities:\n",
      "  - Christopher Nolan\n",
      "  - The Godfather\n",
      "  - Quentin Tarantino\n",
      "  - Science Fiction\n",
      "  - Academy Awards\n"
     ]
    }
   ],
   "source": [
    "# Define the entities we want to track\n",
    "# Each entity maps to its Wikipedia page title variations\n",
    "TRACKED_ENTITIES = {\n",
    "    \"Christopher Nolan\": [\n",
    "        \"Christopher Nolan\",\n",
    "        \"Christopher_Nolan\",\n",
    "        \"Nolan, Christopher\"\n",
    "    ],\n",
    "    \"The Godfather\": [\n",
    "        \"The Godfather\",\n",
    "        \"The_Godfather\",\n",
    "        \"Godfather (film)\",\n",
    "        \"The Godfather (film)\"\n",
    "    ],\n",
    "    \"Quentin Tarantino\": [\n",
    "        \"Quentin Tarantino\",\n",
    "        \"Quentin_Tarantino\",\n",
    "        \"Tarantino\"\n",
    "    ],\n",
    "    \"Science Fiction\": [\n",
    "        \"Science fiction\",\n",
    "        \"Science_fiction\",\n",
    "        \"Science fiction film\",\n",
    "        \"Sci-fi\",\n",
    "        \"Science fiction genre\"\n",
    "    ],\n",
    "    \"Academy Awards\": [\n",
    "        \"Academy Awards\",\n",
    "        \"Academy_Awards\",\n",
    "        \"Oscar\",\n",
    "        \"Oscars\",\n",
    "        \"Academy Award\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a reverse lookup for quick matching\n",
    "TITLE_TO_ENTITY = {}\n",
    "for entity, titles in TRACKED_ENTITIES.items():\n",
    "    for title in titles:\n",
    "        TITLE_TO_ENTITY[title.lower()] = entity\n",
    "\n",
    "print(f\"Tracking {len(TRACKED_ENTITIES)} entities:\")\n",
    "for entity in TRACKED_ENTITIES:\n",
    "    print(f\"  - {entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Output File Configuration\n",
    "\n",
    "Configure the output files for storing events and alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: streaming_output\n",
      "Events file: streaming_output/wiki_events.json\n",
      "Alerts file: streaming_output/wiki_alerts.json\n"
     ]
    }
   ],
   "source": [
    "# Output file paths\n",
    "OUTPUT_DIR = \"streaming_output\"\n",
    "EVENTS_FILE = os.path.join(OUTPUT_DIR, \"wiki_events.json\")\n",
    "ALERTS_FILE = os.path.join(OUTPUT_DIR, \"wiki_alerts.json\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize files with empty arrays if they don't exist\n",
    "for filepath in [EVENTS_FILE, ALERTS_FILE]:\n",
    "    if not os.path.exists(filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump([], f)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Events file: {EVENTS_FILE}\")\n",
    "print(f\"Alerts file: {ALERTS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alert Configuration\n",
    "\n",
    "Define thresholds and rules for generating alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert Configuration:\n",
      "  - Large edit threshold: 500 bytes\n",
      "  - Rapid edit window: 60 seconds\n",
      "  - Rapid edit count: 3 edits\n",
      "  - Alert on bot edits: True\n"
     ]
    }
   ],
   "source": [
    "# Alert configuration\n",
    "ALERT_CONFIG = {\n",
    "    \"large_edit_threshold\": 500,  # Bytes changed to trigger large edit alert\n",
    "    \"rapid_edit_window\": 60,      # Seconds window for rapid edit detection\n",
    "    \"rapid_edit_count\": 3,        # Number of edits in window to trigger alert\n",
    "    \"alert_on_bot\": True          # Whether to alert on bot edits\n",
    "}\n",
    "\n",
    "# Track recent edits for rapid edit detection\n",
    "recent_edits = defaultdict(list)  # entity -> list of timestamps\n",
    "\n",
    "print(\"Alert Configuration:\")\n",
    "print(f\"  - Large edit threshold: {ALERT_CONFIG['large_edit_threshold']} bytes\")\n",
    "print(f\"  - Rapid edit window: {ALERT_CONFIG['rapid_edit_window']} seconds\")\n",
    "print(f\"  - Rapid edit count: {ALERT_CONFIG['rapid_edit_count']} edits\")\n",
    "print(f\"  - Alert on bot edits: {ALERT_CONFIG['alert_on_bot']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Event Processing Functions\n",
    "\n",
    "Core functions for processing events, detecting alerts, and saving data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event processing functions defined.\n"
     ]
    }
   ],
   "source": [
    "def save_event(event_data: dict, filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Append an event to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        event_data: Dictionary containing event information\n",
    "        filepath: Path to the JSON file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read existing data\n",
    "        with open(filepath, 'r') as f:\n",
    "            events = json.load(f)\n",
    "        \n",
    "        # Append new event\n",
    "        events.append(event_data)\n",
    "        \n",
    "        # Write back\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(events, f, indent=2, default=str)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving event: {e}\")\n",
    "\n",
    "\n",
    "def check_for_alerts(event_data: dict, entity: str) -> list:\n",
    "    \"\"\"\n",
    "    Check if an event should trigger any alerts.\n",
    "    \n",
    "    Args:\n",
    "        event_data: Dictionary containing event information\n",
    "        entity: The tracked entity name\n",
    "        \n",
    "    Returns:\n",
    "        List of alert dictionaries\n",
    "    \"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    # Check for large edit\n",
    "    size_change = abs(event_data.get('size_change', 0))\n",
    "    if size_change > ALERT_CONFIG['large_edit_threshold']:\n",
    "        alerts.append({\n",
    "            **event_data,\n",
    "            \"alert_type\": \"LARGE_EDIT\",\n",
    "            \"alert_reason\": f\"Edit changed {size_change} bytes (threshold: {ALERT_CONFIG['large_edit_threshold']})\"\n",
    "        })\n",
    "    \n",
    "    # Check for bot edit\n",
    "    if ALERT_CONFIG['alert_on_bot'] and event_data.get('is_bot', False):\n",
    "        alerts.append({\n",
    "            **event_data,\n",
    "            \"alert_type\": \"BOT_EDIT\",\n",
    "            \"alert_reason\": f\"Automated edit by bot user: {event_data.get('user', 'unknown')}\"\n",
    "        })\n",
    "    \n",
    "    # Check for rapid edits\n",
    "    current_time = datetime.now()\n",
    "    recent_edits[entity].append(current_time)\n",
    "    \n",
    "    # Clean old entries\n",
    "    cutoff_time = current_time - timedelta(seconds=ALERT_CONFIG['rapid_edit_window'])\n",
    "    recent_edits[entity] = [t for t in recent_edits[entity] if t > cutoff_time]\n",
    "    \n",
    "    # Check if rapid edit threshold exceeded\n",
    "    if len(recent_edits[entity]) >= ALERT_CONFIG['rapid_edit_count']:\n",
    "        alerts.append({\n",
    "            **event_data,\n",
    "            \"alert_type\": \"RAPID_EDIT\",\n",
    "            \"alert_reason\": f\"{len(recent_edits[entity])} edits in {ALERT_CONFIG['rapid_edit_window']} seconds\"\n",
    "        })\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "\n",
    "def match_entity(title: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Check if a Wikipedia page title matches any tracked entity.\n",
    "    \n",
    "    Args:\n",
    "        title: Wikipedia page title\n",
    "        \n",
    "    Returns:\n",
    "        Entity name if matched, None otherwise\n",
    "    \"\"\"\n",
    "    title_lower = title.lower()\n",
    "    \n",
    "    # Direct match\n",
    "    if title_lower in TITLE_TO_ENTITY:\n",
    "        return TITLE_TO_ENTITY[title_lower]\n",
    "    \n",
    "    # Partial match (title contains entity name)\n",
    "    for entity_title, entity in TITLE_TO_ENTITY.items():\n",
    "        if entity_title in title_lower or title_lower in entity_title:\n",
    "            return entity\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"Event processing functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metrics Tracking\n",
    "\n",
    "Track aggregate metrics for each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics tracking initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics storage\n",
    "metrics = {\n",
    "    entity: {\n",
    "        \"total_edits\": 0,\n",
    "        \"bot_edits\": 0,\n",
    "        \"human_edits\": 0,\n",
    "        \"total_bytes_changed\": 0,\n",
    "        \"unique_users\": set(),\n",
    "        \"alert_count\": 0,\n",
    "        \"last_edit\": None\n",
    "    }\n",
    "    for entity in TRACKED_ENTITIES\n",
    "}\n",
    "\n",
    "\n",
    "def update_metrics(entity: str, event_data: dict, alerts_generated: int) -> None:\n",
    "    \"\"\"\n",
    "    Update aggregate metrics for an entity.\n",
    "    \n",
    "    Args:\n",
    "        entity: The tracked entity name\n",
    "        event_data: Dictionary containing event information\n",
    "        alerts_generated: Number of alerts generated for this event\n",
    "    \"\"\"\n",
    "    m = metrics[entity]\n",
    "    m[\"total_edits\"] += 1\n",
    "    m[\"total_bytes_changed\"] += abs(event_data.get('size_change', 0))\n",
    "    m[\"alert_count\"] += alerts_generated\n",
    "    m[\"last_edit\"] = event_data.get('timestamp')\n",
    "    \n",
    "    if event_data.get('is_bot', False):\n",
    "        m[\"bot_edits\"] += 1\n",
    "    else:\n",
    "        m[\"human_edits\"] += 1\n",
    "    \n",
    "    user = event_data.get('user')\n",
    "    if user:\n",
    "        m[\"unique_users\"].add(user)\n",
    "\n",
    "\n",
    "def print_metrics() -> None:\n",
    "    \"\"\"\n",
    "    Print current metrics for all tracked entities.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CURRENT METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for entity, m in metrics.items():\n",
    "        print(f\"\\n{entity}:\")\n",
    "        print(f\"  Total edits: {m['total_edits']}\")\n",
    "        print(f\"  Human edits: {m['human_edits']}\")\n",
    "        print(f\"  Bot edits: {m['bot_edits']}\")\n",
    "        print(f\"  Total bytes changed: {m['total_bytes_changed']}\")\n",
    "        print(f\"  Unique users: {len(m['unique_users'])}\")\n",
    "        print(f\"  Alerts generated: {m['alert_count']}\")\n",
    "        print(f\"  Last edit: {m['last_edit'] or 'None yet'}\")\n",
    "\n",
    "\n",
    "print(\"Metrics tracking initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stream Processing Class\n",
    "\n",
    "Main class that connects to Wikimedia EventStreams and processes events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiEventStreamProcessor class defined.\n"
     ]
    }
   ],
   "source": [
    "class WikiEventStreamProcessor:\n",
    "    \"\"\"\n",
    "    Processes Wikipedia Recent Changes event stream and tracks specified entities.\n",
    "    \n",
    "    The Wikimedia EventStreams API provides real-time events for all changes\n",
    "    across Wikimedia projects. We filter for 'recentchange' events on Wikipedia.\n",
    "    \n",
    "    API Documentation: https://wikitech.wikimedia.org/wiki/Event_Platform/EventStreams_HTTP_Service\n",
    "    \"\"\"\n",
    "    \n",
    "    STREAM_URL = \"https://stream.wikimedia.org/v2/stream/recentchange\"\n",
    "    \n",
    "    # Headers to mimic a browser request (required by Wikimedia)\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"IMDBStreamingProject/1.0 (Educational Project; Python/requests)\",\n",
    "        \"Accept\": \"text/event-stream\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "    }\n",
    "    \n",
    "    def __init__(self, duration_seconds: int = 120):\n",
    "        \"\"\"\n",
    "        Initialize the stream processor.\n",
    "        \n",
    "        Args:\n",
    "            duration_seconds: How long to run the stream (default: 2 minutes)\n",
    "        \"\"\"\n",
    "        self.duration_seconds = duration_seconds\n",
    "        self.running = False\n",
    "        self.events_processed = 0\n",
    "        self.events_matched = 0\n",
    "        \n",
    "    def process_event(self, event: dict) -> None:\n",
    "        \"\"\"\n",
    "        Process a single event from the stream.\n",
    "        \n",
    "        Args:\n",
    "            event: Raw event dictionary from EventStreams\n",
    "        \"\"\"\n",
    "        self.events_processed += 1\n",
    "        \n",
    "        # Print progress every 1000 events\n",
    "        if self.events_processed % 1000 == 0:\n",
    "            print(f\"  ... {self.events_processed} events processed, {self.events_matched} matched\")\n",
    "        \n",
    "        # Skip non-edit events\n",
    "        if event.get('type') != 'edit':\n",
    "            return\n",
    "        \n",
    "        # Get page title and check if it matches any tracked entity\n",
    "        title = event.get('title', '')\n",
    "        entity = match_entity(title)\n",
    "        \n",
    "        if entity is None:\n",
    "            return\n",
    "        \n",
    "        self.events_matched += 1\n",
    "        \n",
    "        # Extract relevant data\n",
    "        event_data = {\n",
    "            \"entity\": entity,\n",
    "            \"timestamp\": event.get('meta', {}).get('dt', datetime.now().isoformat()),\n",
    "            \"user\": event.get('user', 'unknown'),\n",
    "            \"is_bot\": event.get('bot', False),\n",
    "            \"title\": title,\n",
    "            \"comment\": event.get('comment', ''),\n",
    "            \"size_change\": event.get('length', {}).get('new', 0) - event.get('length', {}).get('old', 0),\n",
    "            \"wiki\": event.get('wiki', ''),\n",
    "            \"revision_id\": event.get('revision', {}).get('new', 0)\n",
    "        }\n",
    "        \n",
    "        # Save event\n",
    "        save_event(event_data, EVENTS_FILE)\n",
    "        \n",
    "        # Check for alerts\n",
    "        alerts = check_for_alerts(event_data, entity)\n",
    "        for alert in alerts:\n",
    "            save_event(alert, ALERTS_FILE)\n",
    "            print(f\"\\nðŸš¨ ALERT: {alert['alert_type']} for {entity}\")\n",
    "            print(f\"   Reason: {alert['alert_reason']}\")\n",
    "        \n",
    "        # Update metrics\n",
    "        update_metrics(entity, event_data, len(alerts))\n",
    "        \n",
    "        # Print event info\n",
    "        print(f\"\\nâœ“ Event captured for: {entity}\")\n",
    "        print(f\"  Title: {title}\")\n",
    "        print(f\"  User: {event_data['user']} {'(bot)' if event_data['is_bot'] else ''}\")\n",
    "        print(f\"  Change: {event_data['size_change']:+d} bytes\")\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        \"\"\"\n",
    "        Start processing the event stream using a more robust approach.\n",
    "        \"\"\"\n",
    "        print(f\"Starting stream processing for {self.duration_seconds} seconds...\")\n",
    "        print(f\"Tracking entities: {', '.join(TRACKED_ENTITIES.keys())}\")\n",
    "        print(\"\\nConnecting to Wikimedia EventStreams...\")\n",
    "        \n",
    "        self.running = True\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Use requests with stream=True, proper headers, and iter_lines\n",
    "            with requests.get(self.STREAM_URL, stream=True, headers=self.HEADERS, timeout=30) as response:\n",
    "                response.raise_for_status()\n",
    "                print(\"Connected! Listening for events...\\n\")\n",
    "                \n",
    "                for line in response.iter_lines(decode_unicode=True):\n",
    "                    # Check if duration exceeded\n",
    "                    elapsed = time.time() - start_time\n",
    "                    if elapsed > self.duration_seconds:\n",
    "                        print(\"\\n\\nDuration limit reached.\")\n",
    "                        break\n",
    "                    \n",
    "                    if not self.running:\n",
    "                        break\n",
    "                    \n",
    "                    if line:\n",
    "                        # SSE format: lines starting with \"data:\" contain the JSON\n",
    "                        if line.startswith(\"data:\"):\n",
    "                            json_str = line[5:].strip()  # Remove \"data:\" prefix\n",
    "                            if json_str:\n",
    "                                try:\n",
    "                                    data = json.loads(json_str)\n",
    "                                    self.process_event(data)\n",
    "                                except json.JSONDecodeError:\n",
    "                                    continue\n",
    "                        \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"\\nConnection timeout - retrying might help\")\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"\\nConnection error: {e}\")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nStream interrupted by user.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nStream error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            self.running = False\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"STREAM PROCESSING COMPLETE\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Duration: {elapsed:.1f} seconds\")\n",
    "            print(f\"Total events processed: {self.events_processed}\")\n",
    "            print(f\"Events matched to tracked entities: {self.events_matched}\")\n",
    "    \n",
    "    def stop(self) -> None:\n",
    "        \"\"\"\n",
    "        Stop the stream processor.\n",
    "        \"\"\"\n",
    "        self.running = False\n",
    "\n",
    "\n",
    "print(\"WikiEventStreamProcessor class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run the Stream Processor\n",
    "\n",
    "Execute the stream processing job. The processor will run for the specified duration (default: 2 minutes) and capture events for our tracked entities.\n",
    "\n",
    "**Note:** Wikipedia receives thousands of edits per minute globally, but specific page edits are less frequent. You may need to run for a longer duration to capture events for all tracked entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stream processing for 3600 seconds...\n",
      "Tracking entities: Christopher Nolan, The Godfather, Quentin Tarantino, Science Fiction, Academy Awards\n",
      "\n",
      "Connecting to Wikimedia EventStreams...\n",
      "Connected! Listening for events...\n",
      "\n",
      "  ... 1000 events processed, 0 matched\n",
      "  ... 2000 events processed, 0 matched\n",
      "  ... 3000 events processed, 0 matched\n",
      "  ... 4000 events processed, 0 matched\n",
      "  ... 5000 events processed, 0 matched\n",
      "  ... 6000 events processed, 0 matched\n"
     ]
    }
   ],
   "source": [
    "# Create and run the stream processor\n",
    "processor = WikiEventStreamProcessor(duration_seconds=3600)\n",
    "processor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Final Metrics\n",
    "\n",
    "Display the aggregate metrics collected during the stream processing session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CURRENT METRICS\n",
      "============================================================\n",
      "\n",
      "Christopher Nolan:\n",
      "  Total edits: 0\n",
      "  Human edits: 0\n",
      "  Bot edits: 0\n",
      "  Total bytes changed: 0\n",
      "  Unique users: 0\n",
      "  Alerts generated: 0\n",
      "  Last edit: None yet\n",
      "\n",
      "The Godfather:\n",
      "  Total edits: 0\n",
      "  Human edits: 0\n",
      "  Bot edits: 0\n",
      "  Total bytes changed: 0\n",
      "  Unique users: 0\n",
      "  Alerts generated: 0\n",
      "  Last edit: None yet\n",
      "\n",
      "Quentin Tarantino:\n",
      "  Total edits: 0\n",
      "  Human edits: 0\n",
      "  Bot edits: 0\n",
      "  Total bytes changed: 0\n",
      "  Unique users: 0\n",
      "  Alerts generated: 0\n",
      "  Last edit: None yet\n",
      "\n",
      "Science Fiction:\n",
      "  Total edits: 0\n",
      "  Human edits: 0\n",
      "  Bot edits: 0\n",
      "  Total bytes changed: 0\n",
      "  Unique users: 0\n",
      "  Alerts generated: 0\n",
      "  Last edit: None yet\n",
      "\n",
      "Academy Awards:\n",
      "  Total edits: 0\n",
      "  Human edits: 0\n",
      "  Bot edits: 0\n",
      "  Total bytes changed: 0\n",
      "  Unique users: 0\n",
      "  Alerts generated: 0\n",
      "  Last edit: None yet\n"
     ]
    }
   ],
   "source": [
    "# Print final metrics\n",
    "print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Examine Stored Data\n",
    "\n",
    "Load and display the events and alerts stored in the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STORED EVENTS\n",
      "============================================================\n",
      "\n",
      "Total events stored: 0\n"
     ]
    }
   ],
   "source": [
    "# Load and display stored events\n",
    "print(\"=\" * 60)\n",
    "print(\"STORED EVENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with open(EVENTS_FILE, 'r') as f:\n",
    "    events = json.load(f)\n",
    "\n",
    "print(f\"\\nTotal events stored: {len(events)}\")\n",
    "\n",
    "if events:\n",
    "    print(\"\\nSample events:\")\n",
    "    for event in events[:5]:\n",
    "        print(f\"\\n  Entity: {event['entity']}\")\n",
    "        print(f\"  Title: {event['title']}\")\n",
    "        print(f\"  User: {event['user']}\")\n",
    "        print(f\"  Size change: {event['size_change']:+d} bytes\")\n",
    "        print(f\"  Timestamp: {event['timestamp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STORED ALERTS\n",
      "============================================================\n",
      "\n",
      "Total alerts stored: 0\n",
      "\n",
      "No alerts were generated during this session.\n",
      "This could mean:\n",
      "  - No large edits (>500 bytes) occurred\n",
      "  - No bot edits were detected\n",
      "  - No rapid successive edits happened\n"
     ]
    }
   ],
   "source": [
    "# Load and display alerts\n",
    "print(\"=\" * 60)\n",
    "print(\"STORED ALERTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with open(ALERTS_FILE, 'r') as f:\n",
    "    alerts = json.load(f)\n",
    "\n",
    "print(f\"\\nTotal alerts stored: {len(alerts)}\")\n",
    "\n",
    "if alerts:\n",
    "    print(\"\\nAlert details:\")\n",
    "    for alert in alerts:\n",
    "        print(f\"\\n  ðŸš¨ {alert['alert_type']}\")\n",
    "        print(f\"  Entity: {alert['entity']}\")\n",
    "        print(f\"  Reason: {alert['alert_reason']}\")\n",
    "        print(f\"  User: {alert['user']}\")\n",
    "        print(f\"  Timestamp: {alert['timestamp']}\")\n",
    "else:\n",
    "    print(\"\\nNo alerts were generated during this session.\")\n",
    "    print(\"This could mean:\")\n",
    "    print(\"  - No large edits (>500 bytes) occurred\")\n",
    "    print(\"  - No bot edits were detected\")\n",
    "    print(\"  - No rapid successive edits happened\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Event Distribution\n",
    "\n",
    "Create a simple visualization of the events captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVENT DISTRIBUTION\n",
      "============================================================\n",
      "\n",
      "Edits per entity:\n",
      "\n",
      "Christopher Nolan:\n",
      "   (0 edits)\n",
      "\n",
      "The Godfather:\n",
      "   (0 edits)\n",
      "\n",
      "Quentin Tarantino:\n",
      "   (0 edits)\n",
      "\n",
      "Science Fiction:\n",
      "   (0 edits)\n",
      "\n",
      "Academy Awards:\n",
      "   (0 edits)\n"
     ]
    }
   ],
   "source": [
    "# Simple text-based visualization of events per entity\n",
    "print(\"=\" * 60)\n",
    "print(\"EVENT DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "max_bar_length = 40\n",
    "\n",
    "# Get counts per entity\n",
    "entity_counts = {entity: m['total_edits'] for entity, m in metrics.items()}\n",
    "max_count = max(entity_counts.values()) if any(entity_counts.values()) else 1\n",
    "\n",
    "print(\"\\nEdits per entity:\")\n",
    "for entity, count in entity_counts.items():\n",
    "    bar_length = int((count / max_count) * max_bar_length) if max_count > 0 else 0\n",
    "    bar = \"â–ˆ\" * bar_length\n",
    "    print(f\"\\n{entity}:\")\n",
    "    print(f\"  {bar} ({count} edits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Metrics Summary\n",
    "\n",
    "Save a summary of the metrics to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics summary saved to: streaming_output/metrics_summary.json\n",
      "\n",
      "Summary:\n",
      "{\n",
      "  \"Christopher Nolan\": {\n",
      "    \"total_edits\": 0,\n",
      "    \"bot_edits\": 0,\n",
      "    \"human_edits\": 0,\n",
      "    \"total_bytes_changed\": 0,\n",
      "    \"unique_users\": [],\n",
      "    \"unique_user_count\": 0,\n",
      "    \"alert_count\": 0,\n",
      "    \"last_edit\": null\n",
      "  },\n",
      "  \"The Godfather\": {\n",
      "    \"total_edits\": 0,\n",
      "    \"bot_edits\": 0,\n",
      "    \"human_edits\": 0,\n",
      "    \"total_bytes_changed\": 0,\n",
      "    \"unique_users\": [],\n",
      "    \"unique_user_count\": 0,\n",
      "    \"alert_count\": 0,\n",
      "    \"last_edit\": null\n",
      "  },\n",
      "  \"Quentin Tarantino\": {\n",
      "    \"total_edits\": 0,\n",
      "    \"bot_edits\": 0,\n",
      "    \"human_edits\": 0,\n",
      "    \"total_bytes_changed\": 0,\n",
      "    \"unique_users\": [],\n",
      "    \"unique_user_count\": 0,\n",
      "    \"alert_count\": 0,\n",
      "    \"last_edit\": null\n",
      "  },\n",
      "  \"Science Fiction\": {\n",
      "    \"total_edits\": 0,\n",
      "    \"bot_edits\": 0,\n",
      "    \"human_edits\": 0,\n",
      "    \"total_bytes_changed\": 0,\n",
      "    \"unique_users\": [],\n",
      "    \"unique_user_count\": 0,\n",
      "    \"alert_count\": 0,\n",
      "    \"last_edit\": null\n",
      "  },\n",
      "  \"Academy Awards\": {\n",
      "    \"total_edits\": 0,\n",
      "    \"bot_edits\": 0,\n",
      "    \"human_edits\": 0,\n",
      "    \"total_bytes_changed\": 0,\n",
      "    \"unique_users\": [],\n",
      "    \"unique_user_count\": 0,\n",
      "    \"alert_count\": 0,\n",
      "    \"last_edit\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Export metrics summary\n",
    "METRICS_FILE = os.path.join(OUTPUT_DIR, \"metrics_summary.json\")\n",
    "\n",
    "# Convert sets to lists for JSON serialization\n",
    "metrics_export = {}\n",
    "for entity, m in metrics.items():\n",
    "    metrics_export[entity] = {\n",
    "        \"total_edits\": m['total_edits'],\n",
    "        \"bot_edits\": m['bot_edits'],\n",
    "        \"human_edits\": m['human_edits'],\n",
    "        \"total_bytes_changed\": m['total_bytes_changed'],\n",
    "        \"unique_users\": list(m['unique_users']),\n",
    "        \"unique_user_count\": len(m['unique_users']),\n",
    "        \"alert_count\": m['alert_count'],\n",
    "        \"last_edit\": m['last_edit']\n",
    "    }\n",
    "\n",
    "with open(METRICS_FILE, 'w') as f:\n",
    "    json.dump(metrics_export, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Metrics summary saved to: {METRICS_FILE}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(json.dumps(metrics_export, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a complete stream processing pipeline that:\n",
    "\n",
    "1. **Connects** to the Wikimedia EventStreams API to receive real-time Wikipedia edit events\n",
    "2. **Filters** events for five IMDB-related entities (Christopher Nolan, The Godfather, Quentin Tarantino, Science Fiction, Academy Awards)\n",
    "3. **Tracks metrics** including edit counts, bytes changed, unique users, and bot vs. human edits\n",
    "4. **Generates alerts** for:\n",
    "   - Large edits (>500 bytes changed)\n",
    "   - Bot edits\n",
    "   - Rapid successive edits (potential edit wars)\n",
    "5. **Stores data** in two separate JSON files:\n",
    "   - `wiki_events.json` - All captured events\n",
    "   - `wiki_alerts.json` - Alert events requiring attention\n",
    "\n",
    "### Output File Structure\n",
    "\n",
    "```\n",
    "streaming_output/\n",
    "â”œâ”€â”€ wiki_events.json      # All tracked events\n",
    "â”œâ”€â”€ wiki_alerts.json      # Alert events only\n",
    "â””â”€â”€ metrics_summary.json  # Aggregate metrics per entity\n",
    "```\n",
    "\n",
    "### Extending This Pipeline\n",
    "\n",
    "This implementation can be extended to:\n",
    "- Store data in a database (SQLite, PostgreSQL, MongoDB)\n",
    "- Send real alerts via email, Slack, or other notification services\n",
    "- Track additional entities or metrics\n",
    "- Implement more sophisticated anomaly detection algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
