{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cba8e85d-fab2-491c-b846-b1463f8c77d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Engineering with Spark\n",
    "\n",
    "By Tom URBAN & Ethan SMADJA \n",
    "\n",
    "## Lab 3: Structured Streaming\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Connect to the [Databricks Community Edition](https://community.cloud.databricks.com/login.html)\n",
    "- Upload the provided notebook\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Stream the `events` datasets from files\n",
    "- Use Spark Structured Streaming to define the streaming dataframes and process the stream\n",
    "- Visualize how the aggregation results change while new data is coming in\n",
    "- Compare the code for dataframe analysis in batch and streaming mode\n",
    "\n",
    "### Lab resources\n",
    "\n",
    "- Notebook\n",
    "- The data is part of the Databricks workspace: `/databricks-datasets/structured-streaming/events`\n",
    "\n",
    "### Useful links\n",
    "\n",
    "- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "\n",
    "### TO DO\n",
    "\n",
    "1. Explore the dataset in the batch mode\n",
    "2. Do the streaming demo:\n",
    "  - define the streaming dataframe\n",
    "  - define the transformations\n",
    "  - start the stream\n",
    "  - observe the changes in the results\n",
    "3. With the help of the code from the demo, implement streaming example on another dataset  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2852f0ef-95b1-4570-8eaa-5b68c25a5458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Explore the dataset in the batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67579095-543f-4eb6-80fb-8882188e81ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "events_path = \"/databricks-datasets/structured-streaming/events/\"\n",
    "\n",
    "# display the files\n",
    "display(dbutils.fs.ls(events_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d68d25e-ec6d-4342-a9ab-1d1636fa8afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading in batch and Schema display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18697036-d6ad-40cf-80ba-79239174ac56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading in batch\n",
    "events_batch = spark.read.json(events_path)\n",
    "\n",
    "# Schema\n",
    "events_batch.printSchema()\n",
    "\n",
    "# small display\n",
    "events_batch.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61fa2351-f218-42a5-baed-a248747da59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Aggregation and batch !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606c94ba-3a15-427c-8097-a1fed4b089c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lecture en batch : on lit tous les fichiers d’un coup\n",
    "events_batch = spark.read.json(events_path)\n",
    "\n",
    "# Transformation : nombre d’événements par action\n",
    "events_by_action_batch = (\n",
    "    events_batch\n",
    "    .groupBy(\"action\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "# Affichage du résultat final (figé)\n",
    "display(events_by_action_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b29bbb-ed41-4a99-8a38-fc663c9f4136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#2. Démo streaming on events dataset\n",
    "a. define schema help to batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e026d6-a245-4561-9d0b-7c715abc8348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = events_batch.schema\n",
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "494d92ee-56f9-4bb6-a790-ed7271d1cd46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating volumes to stock events\n",
    "![](path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f08563f-8a12-42ab-9f39-5be84c0df4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS workspace.raw_events.events_tmp_25_11_14\n",
    "COMMENT 'Temporary raw events volume for the streaming demo'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS workspace.checkpoints.events_by_action_demo\n",
    "COMMENT 'Checkpoint storage for the streaming demo'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e09a56-0444-4277-8c38-76db1bcb70ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# --- 1. Catalog and schemas ---\n",
    "catalog = \"workspace\"                     # main catalog you are using\n",
    "uc_schema_raw_events = \"raw_events\"       # schema to store raw input data\n",
    "db_schema_checkpoints = \"checkpoints\"     # schema to store streaming checkpoints\n",
    "stream_name = \"events_by_action_demo\"     # logical name of the streaming job\n",
    "\n",
    "# --- 2. Create schemas if they do not exist ---\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{uc_schema_raw_events}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db_schema_checkpoints}\")\n",
    "\n",
    "# --- 3. Create a temporary volume for today's raw data ---\n",
    "raw_events_volume_time = datetime.now()\n",
    "raw_events_volume = f\"events_tmp_{raw_events_volume_time.strftime('%y_%m_%d')}\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS {catalog}.{uc_schema_raw_events}.{raw_events_volume}\n",
    "COMMENT 'Temporary raw events volume for this streaming demo'\n",
    "\"\"\")\n",
    "\n",
    "# --- 4. Define full UC paths for data and checkpoint ---\n",
    "raw_data_path = f\"/Volumes/{catalog}/{uc_schema_raw_events}/{raw_events_volume}\"\n",
    "checkpoint_path = f\"/Volumes/{catalog}/{db_schema_checkpoints}/{stream_name}\"\n",
    "\n",
    "print(\"Raw data path :\", raw_data_path)\n",
    "print(\"Checkpoint path :\", checkpoint_path)\n",
    "\n",
    "# --- 5. Read the data stream ---\n",
    "events_stream = (\n",
    "    spark.readStream\n",
    "        .schema(schema)                   # use the predefined schema\n",
    "        .option(\"maxFilesPerTrigger\", 1)  # process one file at a time\n",
    "        .json(events_path)                # read from the events dataset\n",
    ")\n",
    "\n",
    "# --- 6. Apply transformation ---\n",
    "events_by_action_stream = (\n",
    "    events_stream\n",
    "    .groupBy(\"action\")                    # group by action type\n",
    "    .count()                              # count number of records per action\n",
    ")\n",
    "\n",
    "# --- 7. Start the streaming query and display live results ---\n",
    "display(\n",
    "    events_by_action_stream,\n",
    "    checkpointLocation=checkpoint_path    # store progress in this checkpoint\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02e35d0e-6539-4ea8-8da2-19a4de2271b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Batch vs Streaming – Interpretation:\n",
    "Both approaches produce the same final result because Spark Structured Streaming is built on the same API and execution logic as batch mode.\n",
    "In batch mode, data is processed all at once.\n",
    "In streaming mode, data is processed in small micro-batches, with the state maintained through checkpoints.\n",
    "When the data source is static (as in this lab), the results are identical, but streaming can also handle sources that continuously grow in real time."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB3 Data Engineering with Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
