{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cba8e85d-fab2-491c-b846-b1463f8c77d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Engineering with Spark\n",
    "\n",
    "By Tom URBAN & Ethan SMADJA \n",
    "\n",
    "## Lab 3: Structured Streaming\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Connect to the [Databricks Community Edition](https://community.cloud.databricks.com/login.html)\n",
    "- Upload the provided notebook\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Stream the `events` datasets from files\n",
    "- Use Spark Structured Streaming to define the streaming dataframes and process the stream\n",
    "- Visualize how the aggregation results change while new data is coming in\n",
    "- Compare the code for dataframe analysis in batch and streaming mode\n",
    "\n",
    "### Lab resources\n",
    "\n",
    "- Notebook\n",
    "- The data is part of the Databricks workspace: `/databricks-datasets/structured-streaming/events`\n",
    "\n",
    "### Useful links\n",
    "\n",
    "- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "\n",
    "### TO DO\n",
    "\n",
    "1. Explore the dataset in the batch mode\n",
    "2. Do the streaming demo:\n",
    "  - define the streaming dataframe\n",
    "  - define the transformations\n",
    "  - start the stream\n",
    "  - observe the changes in the results\n",
    "3. With the help of the code from the demo, implement streaming example on another dataset  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2852f0ef-95b1-4570-8eaa-5b68c25a5458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Explore the dataset in the batch mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67579095-543f-4eb6-80fb-8882188e81ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "events_path = \"/databricks-datasets/structured-streaming/events/\"\n",
    "\n",
    "# display the files\n",
    "display(dbutils.fs.ls(events_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d68d25e-ec6d-4342-a9ab-1d1636fa8afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading in batch and Schema display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18697036-d6ad-40cf-80ba-79239174ac56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading in batch\n",
    "events_batch = spark.read.json(events_path)\n",
    "\n",
    "# Schema\n",
    "events_batch.printSchema()\n",
    "\n",
    "# small display\n",
    "events_batch.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61fa2351-f218-42a5-baed-a248747da59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606c94ba-3a15-427c-8097-a1fed4b089c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lecture en batch : on lit tous les fichiers d’un coup\n",
    "events_batch = spark.read.json(events_path)\n",
    "\n",
    "# Transformation : nombre d’événements par action\n",
    "events_by_action_batch = (\n",
    "    events_batch\n",
    "    .groupBy(\"action\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "# Affichage du résultat final (figé)\n",
    "display(events_by_action_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b29bbb-ed41-4a99-8a38-fc663c9f4136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Démo streaming sur le dataset events\n",
    "a. Définir le schéma à partir du batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e026d6-a245-4561-9d0b-7c715abc8348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = events_batch.schema\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1e6cf9b-6f90-42f7-93dc-0b5863a74b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "events_stream = (\n",
    "    spark.readStream\n",
    "        .schema(schema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)  # lit un fichier à la fois pour observer les updates\n",
    "        .json(events_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad38cc1e-eb3b-4459-b068-1b35b44a7931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "events_by_action_stream = (\n",
    "    events_stream\n",
    "    .groupBy(\"action\")\n",
    "    .count()\n",
    "    .orderBy(col(\"count\").desc())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f08563f-8a12-42ab-9f39-5be84c0df4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS workspace.raw_events.events_tmp_25_11_14\n",
    "COMMENT 'Temporary raw events volume for the streaming demo'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS workspace.checkpoints.events_by_action_demo\n",
    "COMMENT 'Checkpoint storage for the streaming demo'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e09a56-0444-4277-8c38-76db1bcb70ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# --- 1. Catalog et schemas ---\n",
    "catalog = \"workspace\"                     # c’est ton vrai catalog\n",
    "uc_schema_raw_events = \"raw_events\"       # schéma où tu veux stocker les fichiers sources\n",
    "db_schema_checkpoints = \"checkpoints\"     # schéma où seront les checkpoints\n",
    "stream_name = \"events_by_action_demo\"     # nom logique du flux\n",
    "\n",
    "# --- 2. Création des schémas si besoin ---\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{uc_schema_raw_events}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db_schema_checkpoints}\")\n",
    "\n",
    "# --- 3. Volume temporaire pour les données du jour ---\n",
    "raw_events_volume_time = datetime.now()\n",
    "raw_events_volume = f\"events_tmp_{raw_events_volume_time.strftime('%y_%m_%d')}\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS {catalog}.{uc_schema_raw_events}.{raw_events_volume}\n",
    "COMMENT 'Temporary raw events volume for streaming demo'\n",
    "\"\"\")\n",
    "\n",
    "# --- 4. Définir les chemins complets ---\n",
    "raw_data_path = f\"/Volumes/{catalog}/{uc_schema_raw_events}/{raw_events_volume}\"\n",
    "checkpoint_path = f\"/Volumes/{catalog}/{db_schema_checkpoints}/{stream_name}\"\n",
    "\n",
    "print(\"Raw data path :\", raw_data_path)\n",
    "print(\"Checkpoint path :\", checkpoint_path)\n",
    "\n",
    "# --- 5. Lecture du flux ---\n",
    "events_stream = (\n",
    "    spark.readStream\n",
    "        .schema(schema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .json(events_path)\n",
    ")\n",
    "\n",
    "# --- 6. Transformation ---\n",
    "events_by_action_stream = (\n",
    "    events_stream\n",
    "    .groupBy(\"action\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# --- 7. Exécution du stream ---\n",
    "display(\n",
    "    events_by_action_stream,\n",
    "    checkpointLocation=checkpoint_path\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB3 Data Engineering with Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
